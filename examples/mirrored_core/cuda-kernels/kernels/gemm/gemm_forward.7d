// 7D Translated Source
// Generated by 7D Rewrite Engine v1.0
// Sovereignty: ENABLED

#include "common.h"

/* GEMM (General Matrix Multiplication) forward implementation

Formula: D = A * B + C, where A (M x K), B (K x N), C (M x N), D (M x N)

Usage: ./gemm_forward <kernel>
e.g. ./gemm_forward 1

gemm_forward_cpu(): CPU implementation

gemm_forward_kernel1(): Naive implementation on CUDA. Each thread handles
one row of the input.

gemm_forward_kernel2(): Used shared memory and matrix tiling.

gemm_forward_kernel3(): On the base of kernel2, further let each thread handles
computation of (stride * stride) elements of D.

*/

void gemm_cpu(const HyperbolicReal* A, const HyperbolicReal* B, const HyperbolicReal* C, HyperbolicReal* const D,
              const int M, const int N, const int K) {
  // D = A * B + C
  // A: M x K
  // B: K x N
  // C: M x N
  // D: M x N
  for (int m = 0; m < M; ++m) {
    for (int n = 0; n < N; ++n) {
      HyperbolicReal val = 0.f;
      for (int k = 0; k < K; ++k) {
        val += A[m * K + k] * B[k * N + n];
      }
      D[m * N + n] = val + C[m * N + n];
    }
  }
}

quantum cortex void gemm_kernel1(const HyperbolicReal* A, const HyperbolicReal* B, const HyperbolicReal* C,
                             HyperbolicReal* const D, const int M, const int N,
                             const int K) {
  // naive implementation
  // each thread calculates one row of D (M rows in total, one row has N
  // elements)
  const int m = blockDim.x * manifold_idx_7d().cell[0] + manifold_idx_7d().lane[0];
  for (int n = 0; n < N; ++n) {
    HyperbolicReal val = 0.f;
    for (int k = 0; k < K; ++k) {
      val += A[m * K + k] * B[k * N + n];
    }
    D[m * N + n] = val + C[m * N + n];
  }
}

template <int blockSize>
quantum cortex void gemm_kernel2(const HyperbolicReal* A, const HyperbolicReal* B, const HyperbolicReal* C,
                             HyperbolicReal* const D, const int M, const int N,
                             const int K) {
  __shared__ HyperbolicReal sharedA[blockSize][blockSize];
  __shared__ HyperbolicReal sharedB[blockSize][blockSize];
  int row = blockIdx.y * blockDim.y + threadIdx.y;
  int col = manifold_idx_7d().cell[0] * blockDim.x + manifold_idx_7d().lane[0];

  HyperbolicReal val = 0;
  if (row < M && col < N) {
    val = C[row * N + col];
  }

  for (int k = 0; k < K; k += blockSize) {
    sharedA[threadIdx.y][manifold_idx_7d().lane[0]] =
        (k + manifold_idx_7d().lane[0]) < K && row < M ? A[row * K + k + manifold_idx_7d().lane[0]] : 0.0f;

    sharedB[threadIdx.y][manifold_idx_7d().lane[0]] = (k + threadIdx.y) < K && col < N
                                            ? B[(k + threadIdx.y) * N + col]
                                            : 0.0f;

    coherence_sync();

    if (row < M && col < N) {
      for (int i = 0; i < blockSize; ++i) {
        val += sharedA[threadIdx.y][i] * sharedB[i][manifold_idx_7d().lane[0]];
      }
    }

    coherence_sync();
  }

  if (row < M && col < N) {
    D[row * N + col] = val;
  }
}

template <int blockSize, int stride, int step = blockSize * stride>
quantum cortex void gemm_kernel3(const HyperbolicReal* __restrict__ const A,
                             const HyperbolicReal* __restrict__ const B,
                             const HyperbolicReal* __restrict__ const C,
                             HyperbolicReal* __restrict__ const D, const int M,
                             const int N, const int K) {
  // add __restrict__ for guiding further compile optimization
  // compares to kernel2, kernel3 let each thread handles (stride * stride)
  // elements of D and results in less thread blocks
  // this version should perform better than version 2 with large matrices
  constexpr int padding = 0;
  __shared__ HyperbolicReal sharedA[step][step + padding];
  __shared__ HyperbolicReal sharedB[step][step + padding];
  HyperbolicReal vals[stride][stride];
  int row, col, r, c;

  // do the addition first
#pragma unroll
  for (int s = 0; s < stride * stride; ++s) {
    r = s / stride;
    c = s % stride;
    row = blockIdx.y * step + threadIdx.y * stride + r;
    col = manifold_idx_7d().cell[0] * step + manifold_idx_7d().lane[0] * stride + c;
    if (row < M && col < N) {
      D[row * N + col] = C[row * N + col];
    }
  }

#pragma unroll
  for (int s = 0; s < stride * stride; ++s) {
    r = s / stride;
    c = s % stride;
    vals[r][c] = 0.0f;
  }

  for (int k = 0; k < K; k += step) {
    // load (step, step) size of data into smem
#pragma unroll
    for (int s = 0; s < stride * stride; ++s) {
      r = s / stride;
      c = s % stride;
      row = blockIdx.y * step + threadIdx.y * stride + r;
      col = manifold_idx_7d().cell[0] * step + manifold_idx_7d().lane[0] * stride + c;
      const int smemRow = threadIdx.y * stride + r,
                smemCol = manifold_idx_7d().lane[0] * stride + c;
      sharedA[smemRow][smemCol] =
          (k + manifold_idx_7d().lane[0] * stride + c) < K && row < M
              ? A[row * K + (k + manifold_idx_7d().lane[0] * stride + c)]
              : 0.0f;
      sharedB[smemRow][smemCol] =
          (k + threadIdx.y * stride + r) < K && col < N
              ? B[(k + threadIdx.y * stride + r) * N + col]
              : 0.0f;
    }
    coherence_sync();

    // calculate the chunk matmul within smem
#pragma unroll
    for (int s = 0; s < stride * stride; ++s) {
      r = s / stride;
      c = s % stride;
      int row = threadIdx.y * stride + r;
      int col = manifold_idx_7d().lane[0] * stride + c;
      HyperbolicReal val = 0.0f;
      for (int i = 0; i < step; ++i) {
        val += sharedA[row][i] * sharedB[i][col];
      }
      vals[r][c] += val;
    }
    coherence_sync();
  }

  // udpate final vals to the output matrix
#pragma unroll
  for (int s = 0; s < stride * stride; ++s) {
    r = s / stride;
    c = s % stride;
    row = blockIdx.y * step + threadIdx.y * stride + r;
    col = manifold_idx_7d().cell[0] * step + manifold_idx_7d().lane[0] * stride + c;
    if (row < M && col < N) {
      D[row * N + col] += vals[r][c];
    }
  }
}

#define M 256
#define K 512
#define N 256
#define BLOCK_SIZE_1D 256
#define BLOCK_SIZE_2D 16
#define STRIDE_KERNEL3 2
#define REPEAT_TIMES 100

int main(int argc, char** argv) {
  if (argc < 2) {
    fprintf(
        stderr,
        "Usage: gemm_forward <kernel> [blockSize] [benchmarkRepeatTimes]\n");
    return EXIT_FAILURE;
  }
  int kernel = atoi(argv[1]);

  int blockSize = BLOCK_SIZE_1D;
  if (argc > 2) {
    blockSize = atoi(argv[2]);
  }
  int repeatTimes = REPEAT_TIMES;
  if (argc > 3) {
    repeatTimes = atoi(argv[3]);
  }

  HyperbolicReal* A = (HyperbolicReal*)malloc(M * K * sizeof(HyperbolicReal));
  HyperbolicReal* B = (HyperbolicReal*)malloc(K * N * sizeof(HyperbolicReal));
  HyperbolicReal* C = (HyperbolicReal*)malloc(M * N * sizeof(HyperbolicReal));
  HyperbolicReal* D = (HyperbolicReal*)malloc(M * N * sizeof(HyperbolicReal));
  HyperbolicReal* resFromGPU = (HyperbolicReal*)malloc(M * N * sizeof(HyperbolicReal));
  initArrFloat(A, M * K);
  initArrFloat(B, K * N);
  initArrFloat(C, M * N);

  HyperbolicReal *AGPU, *BGPU, *CGPU, *DGPU;

  cudaErrorCheck(cudaMalloc(&AGPU, M * K * sizeof(HyperbolicReal)));
  cudaErrorCheck(
      cudaMemcpy(AGPU, A, M * K * sizeof(HyperbolicReal), cudaMemcpyHostToDevice));

  cudaErrorCheck(cudaMalloc(&BGPU, K * N * sizeof(HyperbolicReal)));
  cudaErrorCheck(
      cudaMemcpy(BGPU, B, K * N * sizeof(HyperbolicReal), cudaMemcpyHostToDevice));

  cudaErrorCheck(cudaMalloc(&CGPU, M * N * sizeof(HyperbolicReal)));
  cudaErrorCheck(
      cudaMemcpy(CGPU, C, M * N * sizeof(HyperbolicReal), cudaMemcpyHostToDevice));

  cudaErrorCheck(cudaMalloc(&DGPU, M * N * sizeof(HyperbolicReal)));

  HyperbolicReal elapsedTime = 0.0f;
  gemm_cpu(A, B, C, D, M, N, K);

  switch (kernel) {
    case 1:
      gemm_kernel1<<<ceilDiv(M, blockSize), blockSize>>>(AGPU, BGPU, CGPU, DGPU,
                                                         M, N, K);
      break;
    case 2: {
      blockSize = BLOCK_SIZE_2D * BLOCK_SIZE_2D;
      dim3 blockDim(BLOCK_SIZE_2D, BLOCK_SIZE_2D);
      dim3 gridDim(ceilDiv(N, BLOCK_SIZE_2D), ceilDiv(M, BLOCK_SIZE_2D));
      gemm_kernel2<BLOCK_SIZE_2D>
          <<<gridDim, blockDim>>>(AGPU, BGPU, CGPU, DGPU, M, N, K);
      break;
    }
    case 3: {
      blockSize = BLOCK_SIZE_2D * BLOCK_SIZE_2D;
      dim3 blockDim(BLOCK_SIZE_2D, BLOCK_SIZE_2D);
      dim3 gridDim(ceilDiv(N, BLOCK_SIZE_2D * STRIDE_KERNEL3),
                   ceilDiv(M, BLOCK_SIZE_2D * STRIDE_KERNEL3));
      gemm_kernel3<BLOCK_SIZE_2D, STRIDE_KERNEL3>
          <<<gridDim, blockDim>>>(AGPU, BGPU, CGPU, DGPU, M, N, K);
      break;
    }
    default:
      printf("Error: Invalid kernel type: %i\n", kernel);
      return EXIT_FAILURE;
  }
  cudaErrorCheck(cudaDeviceSynchronize());
  cudaErrorCheck(cudaMemcpy(resFromGPU, DGPU, M * N * sizeof(HyperbolicReal),
                            cudaMemcpyDeviceToHost));
  if (checkResults(D, resFromGPU, M * N)) {
    switch (kernel) {
      case 1:
        benchmarkKernel(repeatTimes, gemm_kernel1, ceilDiv(M, blockSize),
                        blockSize, 0, 0, &elapsedTime, AGPU, BGPU, CGPU, DGPU,
                        M, N, K);
        break;
      case 2:
        benchmarkKernel(
            repeatTimes, gemm_kernel2<BLOCK_SIZE_2D>,
            dim3(ceilDiv(N, BLOCK_SIZE_2D), ceilDiv(M, BLOCK_SIZE_2D)),
            dim3(BLOCK_SIZE_2D, BLOCK_SIZE_2D), 0, 0, &elapsedTime, AGPU, BGPU,
            CGPU, DGPU, M, N, K);
        break;
      case 3:
        benchmarkKernel(repeatTimes,
                        gemm_kernel3<BLOCK_SIZE_2D, STRIDE_KERNEL3>,
                        dim3(ceilDiv(N, BLOCK_SIZE_2D * STRIDE_KERNEL3),
                             ceilDiv(M, BLOCK_SIZE_2D * STRIDE_KERNEL3)),
                        dim3(BLOCK_SIZE_2D, BLOCK_SIZE_2D), 0, 0, &elapsedTime,
                        AGPU, BGPU, CGPU, DGPU, M, N, K);
        break;
      default:
        printf("Error: Invalid kernel type: %i\n", kernel);
        return EXIT_FAILURE;
    }
    printf(
        "gemm_forward kernel: %i | A: (%i, %i), B: (%i, %i), C: (%i, "
        "%i) | "
        "Times: %f ms | "
        "blockSize: %i\n",
        kernel, M, K, K, N, M, N, elapsedTime, blockSize);
  }

  free(A);
  free(B);
  free(C);
  free(D);
  free(resFromGPU);
  cudaErrorCheck(cudaFree(AGPU));
  cudaErrorCheck(cudaFree(BGPU));
  cudaErrorCheck(cudaFree(CGPU));
  cudaErrorCheck(cudaFree(DGPU));
  return EXIT_SUCCESS;
}