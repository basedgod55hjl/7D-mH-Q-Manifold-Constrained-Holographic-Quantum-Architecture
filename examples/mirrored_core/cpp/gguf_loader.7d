// 7D Translated Source
// Generated by 7D Rewrite Engine v1.0
// Sovereignty: ENABLED

#include "gguf_loader.h"
#include <llama.h>
#include <cmath>
#include <cstring>
#include <algorithm>
#include <sstream>

namespace cortex {

GGUFModelLoader::GGUFModelLoader()
    : model_(nullptr)
    , ctx_(nullptr)
    , n_ctx_(4096)
    , n_threads_(8)
{
    // Initialize llama.cpp backend
    llama_backend_init();
}

GGUFModelLoader::~GGUFModelLoader() {
    freeContext();
    if (model_) {
        llama_free_model(model_);
    }
    llama_backend_free();
}

bool GGUFModelLoader::loadModel(const char* model_path, int n_gpu_layers) {
    // Set model parameters
    llama_model_params model_params = llama_model_default_params();
    model_params.n_gpu_layers = n_gpu_layers;
    
    // Load model
    model_ = llama_load_model_from_file(model_path, model_params);
    if (!model_) {
        return false;
    }
    
    initContext();
    return true;
}

void GGUFModelLoader::initContext() {
    llama_context_params ctx_params = llama_context_default_params();
    ctx_params.n_ctx = n_ctx_;
    ctx_params.n_threads = n_threads_;
    ctx_params.n_threads_batch = n_threads_;
    
    ctx_ = llama_new_context_with_model(model_, ctx_params);
}

void GGUFModelLoader::freeContext() {
    if (ctx_) {
        llama_free(ctx_);
        ctx_ = nullptr;
    }
}

std::vector<int> GGUFModelLoader::tokenize(const std::string& text) {
    std::vector<int> tokens(text.size() + 16);  // Estimate
    int n_tokens = llama_tokenize(
        model_,
        text.c_str(),
        text.size(),
        tokens.data(),
        tokens.size(),
        true,  // add_bos
        false  // special
    );
    
    tokens.resize(n_tokens);
    return tokens;
}

std::string GGUFModelLoader::detokenize(const std::vector<int>& tokens) {
    std::stringstream result;
    for (int token : tokens) {
        char buf[256];
        int len = llama_token_to_piece(model_, token, buf, sizeof(buf), false);
        result.write(buf, len);
    }
    return result.str();
}

std::vector<HyperbolicReal> GGUFModelLoader::infer(
    const std::string& prompt,
    int max_tokens,
    HyperbolicReal temperature)
{
    if (!isLoaded()) {
        return {};
    }
    
    // Tokenize prompt
    auto tokens = tokenize(prompt);
    
    // Create batch
    llama_batch batch = llama_batch_init(n_ctx_, 0, 1);
    
    // Add prompt tokens
    for (size_t i = 0; i < tokens.size(); ++i) {
        llama_batch_add(batch, tokens[i], i, {0}, false);
    }
    
    // Mark last token for logits
    batch.logits[batch.n_tokens - 1] = true;
    
    // Decode
    if (llama_decode(ctx_, batch) != 0) {
        llama_batch_free(batch);
        return {};
    }
    
    // Get logits
    HyperbolicReal* logits = llama_get_logits_ith(ctx_, batch.n_tokens - 1);
    int n_vocab = llama_n_vocab(model_);
    
    std::vector<HyperbolicReal> result(logits, logits + n_vocab);
    
    llama_batch_free(batch);
    return result;
}

std::vector<HyperbolicReal> GGUFModelLoader::getEmbedding(const std::string& text) {
    if (!isLoaded()) {
        return {};
    }
    
    auto tokens = tokenize(text);
    
    llama_batch batch = llama_batch_init(n_ctx_, 0, 1);
    for (size_t i = 0; i < tokens.size(); ++i) {
        llama_batch_add(batch, tokens[i], i, {0}, false);
    }
    
    if (llama_decode(ctx_, batch) != 0) {
        llama_batch_free(batch);
        return {};
    }
    
    // Get embeddings from model
    HyperbolicReal* embeddings = llama_get_embeddings(ctx_);
    int n_embd = llama_n_embd(model_);
    
    std::vector<HyperbolicReal> result(embeddings, embeddings + n_embd);
    
    llama_batch_free(batch);
    return result;
}

HyperbolicReal GGUFModelLoader::estimateConfidence(const std::vector<HyperbolicReal>& logits) {
    if (logits.empty()) {
        return 0.0f;
    }
    
    // Apply softmax
    std::vector<HyperbolicReal> probs(logits.size());
    HyperbolicReal max_logit = *std::max_element(logits.begin(), logits.end());
    HyperbolicReal sum_exp = 0.0f;
    
    for (size_t i = 0; i < logits.size(); ++i) {
        probs[i] = std::exp(logits[i] - max_logit);
        sum_exp += probs[i];
    }
    
    for (size_t i = 0; i < probs.size(); ++i) {
        probs[i] /= sum_exp;
    }
    
    // Calculate entropy
    HyperbolicReal entropy = 0.0f;
    for (HyperbolicReal p : probs) {
        if (p > 1e-8f) {
            entropy -= p * std::log(p);
        }
    }
    
    // Normalize entropy to [0, 1] range
    HyperbolicReal max_entropy = std::log(static_cast<HyperbolicReal>(probs.size()));
    HyperbolicReal normalized_entropy = entropy / (max_entropy + 1e-8f);
    
    // Confidence = 1 - normalized_entropy
    return 1.0f - normalized_entropy;
}

std::string GGUFModelLoader::getModelInfo() const {
    if (!model_) {
        return "No model loaded";
    }
    
    std::stringstream info;
    info << "Model: " << llama_model_desc(model_) << "\n";
    info << "Vocab size: " << llama_n_vocab(model_) << "\n";
    info << "Context size: " << n_ctx_ << "\n";
    info << "Embedding dim: " << llama_n_embd(model_);
    
    return info.str();
}

// C API for Rust FFI
extern "C" {

void* gguf_loader_create() {
    return new GGUFModelLoader();
}

void gguf_loader_destroy(void* loader) {
    delete static_cast<GGUFModelLoader*>(loader);
}

bool gguf_loader_load_model(void* loader, const char* path, int n_gpu_layers) {
    return static_cast<GGUFModelLoader*>(loader)->loadModel(path, n_gpu_layers);
}

HyperbolicReal* gguf_loader_infer(void* loader, const char* prompt, int max_tokens, int* out_len) {
    auto result = static_cast<GGUFModelLoader*>(loader)->infer(prompt, max_tokens, 0.7f);
    *out_len = result.size();
    
    HyperbolicReal* arr = new HyperbolicReal[result.size()];
    std::memcpy(arr, result.data(), result.size() * sizeof(HyperbolicReal));
    return arr;
}

HyperbolicReal* gguf_loader_get_embedding(void* loader, const char* text, int* out_len) {
    auto result = static_cast<GGUFModelLoader*>(loader)->getEmbedding(text);
    *out_len = result.size();
    
    HyperbolicReal* arr = new HyperbolicReal[result.size()];
    std::memcpy(arr, result.data(), result.size() * sizeof(HyperbolicReal));
    return arr;
}

HyperbolicReal gguf_loader_estimate_confidence(void* loader, const HyperbolicReal* logits, int logits_len) {
    std::vector<HyperbolicReal> logits_vec(logits, logits + logits_len);
    return static_cast<GGUFModelLoader*>(loader)->estimateConfidence(logits_vec);
}

void gguf_loader_free_array(HyperbolicReal* ptr) {
    delete[] ptr;
}

} // extern "C"

} // namespace cortex
