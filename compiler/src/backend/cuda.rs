//! # 7D Crystal CUDA Backend
//!
//! Generates NVIDIA CUDA PTX code from 7D Crystal IR.
//!
//! ## Features
//!
//! - PTX generation for SM 7.0+ (Volta and newer)
//! - 7D manifold operations as CUDA kernels
//! - Φ-ratio optimization for tensor cores
//! - S² stability enforcement
//!
//! ## Kernel Types
//!
//! - `manifold_project_7d`: Projects vectors to Poincaré ball
//! - `holographic_fold_7d`: Holographic interference patterns
//! - `quantum_superpose_7d`: Quantum superposition operations

use crate::ir::{IRBlock7D, IR7D};
use crate::{DIMS, PHI, PHI_INV, S2_STABILITY};

// ═══════════════════════════════════════════════════════════════════════════════
// PTX GENERATION
// ═══════════════════════════════════════════════════════════════════════════════

/// Generate CUDA PTX code from IR blocks.
///
/// # Arguments
///
/// * `blocks` - The IR blocks to compile.
/// * `compute_capability` - Target GPU architecture (major, minor).
///
/// # Returns
///
/// PTX assembly code as a string.
pub fn generate_ptx(
    blocks: &[IRBlock7D],
    compute_capability: (u32, u32),
) -> Result<String, String> {
    let mut output = String::new();

    // PTX header
    output.push_str(&format!(
        "// 7D Crystal CUDA PTX - Target SM {}.{}\n",
        compute_capability.0, compute_capability.1
    ));
    output.push_str("// Generated by 7D Crystal Compiler v2.0.0\n");
    output.push_str("// (c) 2025-2026 Sir Charles Spikes\n\n");

    output.push_str(&format!(
        ".version 7.0\n.target sm_{}{}\n.address_size 64\n\n",
        compute_capability.0, compute_capability.1
    ));

    // Global constants
    output.push_str("// Sacred constants\n");
    output.push_str(&format!(".global .align 8 .f64 PHI = {:e};\n", PHI));
    output.push_str(&format!(".global .align 8 .f64 PHI_INV = {:e};\n", PHI_INV));
    output.push_str(&format!(
        ".global .align 8 .f64 S2_BOUND = {:e};\n",
        S2_STABILITY
    ));
    output.push_str(&format!(".global .align 4 .u32 DIMS = {};\n\n", DIMS));

    // Built-in kernels
    output.push_str(&generate_builtin_kernels());

    // Generate kernels from IR blocks
    for block in blocks {
        output.push_str(&generate_kernel_from_block(block)?);
    }

    Ok(output)
}

/// Generate HIP-compatible code (similar to CUDA).
pub fn generate_hip(blocks: &[IRBlock7D]) -> Result<String, String> {
    let mut output = String::new();

    // HIP header
    output.push_str("// 7D Crystal HIP Code - AMD ROCm\n");
    output.push_str("// Generated by 7D Crystal Compiler v2.0.0\n");
    output.push_str("// (c) 2025-2026 Sir Charles Spikes\n\n");

    output.push_str("#include <hip/hip_runtime.h>\n");
    output.push_str("#include <hip/hip_fp16.h>\n\n");

    // Constants
    output.push_str("// Sacred constants\n");
    output.push_str(&format!("__constant__ double PHI = {:e};\n", PHI));
    output.push_str(&format!("__constant__ double PHI_INV = {:e};\n", PHI_INV));
    output.push_str(&format!(
        "__constant__ double S2_BOUND = {:e};\n",
        S2_STABILITY
    ));
    output.push_str(&format!("__constant__ int DIMS = {};\n\n", DIMS));

    // Vector7D struct
    output.push_str(VECTOR7D_HIP_STRUCT);

    // Built-in kernels
    output.push_str(&generate_hip_kernels());

    // Generate kernels from IR blocks
    for block in blocks {
        output.push_str(&generate_hip_kernel_from_block(block)?);
    }

    Ok(output)
}

// ═══════════════════════════════════════════════════════════════════════════════
// BUILT-IN KERNELS
// ═══════════════════════════════════════════════════════════════════════════════

fn generate_builtin_kernels() -> String {
    let mut output = String::new();

    // 7D Norm calculation
    output.push_str(
        r#"
// Calculate 7D vector norm
.func (.reg .f64 result) norm_7d (
    .reg .f64 v0, .reg .f64 v1, .reg .f64 v2, .reg .f64 v3,
    .reg .f64 v4, .reg .f64 v5, .reg .f64 v6
) {
    .reg .f64 sum, temp;
    mul.f64 temp, v0, v0;
    mov.f64 sum, temp;
    fma.rn.f64 sum, v1, v1, sum;
    fma.rn.f64 sum, v2, v2, sum;
    fma.rn.f64 sum, v3, v3, sum;
    fma.rn.f64 sum, v4, v4, sum;
    fma.rn.f64 sum, v5, v5, sum;
    fma.rn.f64 sum, v6, v6, sum;
    sqrt.rn.f64 result, sum;
    ret;
}

"#,
    );

    // 7D Manifold projection
    output.push_str(
        r#"
// Project vector to Poincaré ball (enforce S² bound)
.visible .entry manifold_project_7d (
    .param .u64 input_ptr,
    .param .u64 output_ptr,
    .param .f64 curvature,
    .param .u64 count
) {
    .reg .u64 idx, base_in, base_out;
    .reg .f64 v<7>, norm, scale, denom;
    .reg .pred p;
    
    // Calculate thread index
    mov.u32 %r1, %ctaid.x;
    mov.u32 %r2, %ntid.x;
    mov.u32 %r3, %tid.x;
    mad.lo.u32 %r4, %r1, %r2, %r3;
    cvt.u64.u32 idx, %r4;
    
    // Bounds check
    ld.param.u64 %rd1, [count];
    setp.ge.u64 p, idx, %rd1;
    @p bra EXIT;
    
    // Load input pointer and calculate offset
    ld.param.u64 base_in, [input_ptr];
    ld.param.u64 base_out, [output_ptr];
    shl.b64 %rd2, idx, 6;  // idx * 64 (8 doubles, but we use 7)
    add.u64 base_in, base_in, %rd2;
    add.u64 base_out, base_out, %rd2;
    
    // Load 7D vector
    ld.global.f64 v0, [base_in];
    ld.global.f64 v1, [base_in+8];
    ld.global.f64 v2, [base_in+16];
    ld.global.f64 v3, [base_in+24];
    ld.global.f64 v4, [base_in+32];
    ld.global.f64 v5, [base_in+40];
    ld.global.f64 v6, [base_in+48];
    
    // Calculate norm
    call (norm), norm_7d, (v0, v1, v2, v3, v4, v5, v6);
    
    // Calculate projection scale: 1 / (1 + norm + PHI_INV + |curvature|)
    ld.param.f64 %fd1, [curvature];
    abs.f64 %fd2, %fd1;
    ld.global.f64 %fd3, [PHI_INV];
    add.f64 denom, norm, %fd3;
    add.f64 denom, denom, %fd2;
    add.f64 denom, denom, 1.0;
    rcp.rn.f64 scale, denom;
    
    // Scale vector
    mul.f64 v0, v0, scale;
    mul.f64 v1, v1, scale;
    mul.f64 v2, v2, scale;
    mul.f64 v3, v3, scale;
    mul.f64 v4, v4, scale;
    mul.f64 v5, v5, scale;
    mul.f64 v6, v6, scale;
    
    // Store result
    st.global.f64 [base_out], v0;
    st.global.f64 [base_out+8], v1;
    st.global.f64 [base_out+16], v2;
    st.global.f64 [base_out+24], v3;
    st.global.f64 [base_out+32], v4;
    st.global.f64 [base_out+40], v5;
    st.global.f64 [base_out+48], v6;
    
EXIT:
    ret;
}

"#,
    );

    // Holographic fold kernel
    output.push_str(
        r#"
// Holographic fold (interference pattern merge)
.visible .entry holographic_fold_7d (
    .param .u64 pattern1_ptr,
    .param .u64 pattern2_ptr,
    .param .u64 output_ptr,
    .param .u32 phases,
    .param .u64 count
) {
    .reg .u64 idx, base_p1, base_p2, base_out;
    .reg .f64 p1<7>, p2<7>, out<7>, phase_scale;
    .reg .pred p;
    
    // Calculate thread index
    mov.u32 %r1, %ctaid.x;
    mov.u32 %r2, %ntid.x;
    mov.u32 %r3, %tid.x;
    mad.lo.u32 %r4, %r1, %r2, %r3;
    cvt.u64.u32 idx, %r4;
    
    // Bounds check
    ld.param.u64 %rd1, [count];
    setp.ge.u64 p, idx, %rd1;
    @p bra FOLD_EXIT;
    
    // Load pointers
    ld.param.u64 base_p1, [pattern1_ptr];
    ld.param.u64 base_p2, [pattern2_ptr];
    ld.param.u64 base_out, [output_ptr];
    shl.b64 %rd2, idx, 6;
    add.u64 base_p1, base_p1, %rd2;
    add.u64 base_p2, base_p2, %rd2;
    add.u64 base_out, base_out, %rd2;
    
    // Calculate phase scale based on number of phases
    ld.param.u32 %r5, [phases];
    cvt.rn.f64.u32 phase_scale, %r5;
    rcp.rn.f64 phase_scale, phase_scale;
    
    // Load patterns
    ld.global.f64 p10, [base_p1];
    ld.global.f64 p11, [base_p1+8];
    ld.global.f64 p12, [base_p1+16];
    ld.global.f64 p13, [base_p1+24];
    ld.global.f64 p14, [base_p1+32];
    ld.global.f64 p15, [base_p1+40];
    ld.global.f64 p16, [base_p1+48];
    
    ld.global.f64 p20, [base_p2];
    ld.global.f64 p21, [base_p2+8];
    ld.global.f64 p22, [base_p2+16];
    ld.global.f64 p23, [base_p2+24];
    ld.global.f64 p24, [base_p2+32];
    ld.global.f64 p25, [base_p2+40];
    ld.global.f64 p26, [base_p2+48];
    
    // Interference pattern: (p1 + p2) * phase_scale with Φ modulation
    ld.global.f64 %fd1, [PHI_INV];
    add.f64 out0, p10, p20;
    add.f64 out1, p11, p21;
    add.f64 out2, p12, p22;
    add.f64 out3, p13, p23;
    add.f64 out4, p14, p24;
    add.f64 out5, p15, p25;
    add.f64 out6, p16, p26;
    
    mul.f64 out0, out0, phase_scale;
    mul.f64 out1, out1, phase_scale;
    mul.f64 out2, out2, phase_scale;
    mul.f64 out3, out3, phase_scale;
    mul.f64 out4, out4, phase_scale;
    mul.f64 out5, out5, phase_scale;
    mul.f64 out6, out6, phase_scale;
    
    // Store result
    st.global.f64 [base_out], out0;
    st.global.f64 [base_out+8], out1;
    st.global.f64 [base_out+16], out2;
    st.global.f64 [base_out+24], out3;
    st.global.f64 [base_out+32], out4;
    st.global.f64 [base_out+40], out5;
    st.global.f64 [base_out+48], out6;

FOLD_EXIT:
    ret;
}

"#,
    );

    // Phi ratio verification kernel
    output.push_str(
        r#"
// Verify Φ-ratio preservation
.visible .entry verify_phi_ratio_7d (
    .param .u64 input_ptr,
    .param .u64 result_ptr,
    .param .f64 tolerance,
    .param .u64 count
) {
    .reg .u64 idx, base_in;
    .reg .f64 v<7>, ratio, diff, tol;
    .reg .u32 valid;
    .reg .pred p, p_valid;
    
    // Calculate thread index
    mov.u32 %r1, %ctaid.x;
    mov.u32 %r2, %ntid.x;
    mov.u32 %r3, %tid.x;
    mad.lo.u32 %r4, %r1, %r2, %r3;
    cvt.u64.u32 idx, %r4;
    
    // Bounds check
    ld.param.u64 %rd1, [count];
    setp.ge.u64 p, idx, %rd1;
    @p bra PHI_EXIT;
    
    // Load input
    ld.param.u64 base_in, [input_ptr];
    shl.b64 %rd2, idx, 6;
    add.u64 base_in, base_in, %rd2;
    
    ld.global.f64 v0, [base_in];
    ld.global.f64 v1, [base_in+8];
    ld.global.f64 v2, [base_in+16];
    ld.global.f64 v3, [base_in+24];
    ld.global.f64 v4, [base_in+32];
    ld.global.f64 v5, [base_in+40];
    ld.global.f64 v6, [base_in+48];
    
    ld.param.f64 tol, [tolerance];
    ld.global.f64 %fd1, [PHI];
    mov.u32 valid, 1;
    
    // Check ratio v1/v0 ≈ Φ
    div.rn.f64 ratio, v1, v0;
    sub.f64 diff, ratio, %fd1;
    abs.f64 diff, diff;
    setp.gt.f64 p_valid, diff, tol;
    @p_valid mov.u32 valid, 0;
    
    // Check ratio v2/v1 ≈ Φ
    div.rn.f64 ratio, v2, v1;
    sub.f64 diff, ratio, %fd1;
    abs.f64 diff, diff;
    setp.gt.f64 p_valid, diff, tol;
    @p_valid mov.u32 valid, 0;
    
    // ... (continue for all consecutive pairs)
    
    // Store result
    ld.param.u64 %rd3, [result_ptr];
    shl.b64 %rd4, idx, 2;
    add.u64 %rd3, %rd3, %rd4;
    st.global.u32 [%rd3], valid;

PHI_EXIT:
    ret;
}

"#,
    );

    output
}

// ═══════════════════════════════════════════════════════════════════════════════
// HIP KERNELS
// ═══════════════════════════════════════════════════════════════════════════════

const VECTOR7D_HIP_STRUCT: &str = r#"
// 7D Vector structure
struct alignas(64) Vector7D {
    double coords[7];
    
    __device__ __host__ double norm() const {
        double sum = 0.0;
        #pragma unroll
        for (int i = 0; i < 7; i++) {
            sum += coords[i] * coords[i];
        }
        return sqrt(sum);
    }
    
    __device__ __host__ Vector7D operator+(const Vector7D& other) const {
        Vector7D result;
        #pragma unroll
        for (int i = 0; i < 7; i++) {
            result.coords[i] = coords[i] + other.coords[i];
        }
        return result;
    }
    
    __device__ __host__ Vector7D operator*(double scalar) const {
        Vector7D result;
        #pragma unroll
        for (int i = 0; i < 7; i++) {
            result.coords[i] = coords[i] * scalar;
        }
        return result;
    }
};

"#;

fn generate_hip_kernels() -> String {
    let mut output = String::new();

    // Manifold projection kernel
    output.push_str(
        r#"
// Project vector to Poincaré ball (HIP version)
__global__ void manifold_project_7d(
    const Vector7D* __restrict__ input,
    Vector7D* __restrict__ output,
    double curvature,
    size_t count
) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= count) return;
    
    Vector7D v = input[idx];
    double norm = v.norm();
    
    // Project: v / (1 + norm + PHI_INV + |curvature|)
    double denom = 1.0 + norm + PHI_INV + fabs(curvature);
    double scale = 1.0 / denom;
    
    #pragma unroll
    for (int i = 0; i < 7; i++) {
        output[idx].coords[i] = v.coords[i] * scale;
    }
}

// Holographic fold kernel (HIP version)
__global__ void holographic_fold_7d(
    const Vector7D* __restrict__ pattern1,
    const Vector7D* __restrict__ pattern2,
    Vector7D* __restrict__ output,
    int phases,
    size_t count
) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= count) return;
    
    Vector7D p1 = pattern1[idx];
    Vector7D p2 = pattern2[idx];
    double phase_scale = 1.0 / (double)phases;
    
    #pragma unroll
    for (int i = 0; i < 7; i++) {
        // Interference pattern with Φ modulation
        output[idx].coords[i] = (p1.coords[i] + p2.coords[i]) * phase_scale * PHI_INV;
    }
}

// Quantum superposition kernel (HIP version)
__global__ void quantum_superpose_7d(
    const Vector7D* __restrict__ states,
    const double* __restrict__ weights,
    Vector7D* __restrict__ output,
    int num_states,
    size_t count
) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= count) return;
    
    Vector7D result = {0};
    
    for (int s = 0; s < num_states; s++) {
        Vector7D state = states[s * count + idx];
        double weight = weights[s];
        
        #pragma unroll
        for (int i = 0; i < 7; i++) {
            result.coords[i] += state.coords[i] * weight;
        }
    }
    
    output[idx] = result;
}

"#,
    );

    output
}

// ═══════════════════════════════════════════════════════════════════════════════
// KERNEL GENERATION FROM IR
// ═══════════════════════════════════════════════════════════════════════════════

fn generate_kernel_from_block(block: &IRBlock7D) -> Result<String, String> {
    let mut output = String::new();

    output.push_str(&format!("// Kernel: {}\n", block.name));
    output.push_str(&format!(".visible .entry {} (\n", block.name));
    output.push_str("    .param .u64 input_ptr,\n");
    output.push_str("    .param .u64 output_ptr,\n");
    output.push_str("    .param .u64 count\n");
    output.push_str(") {\n");

    // Register declarations
    output.push_str("    .reg .u64 idx, base_in, base_out;\n");
    output.push_str("    .reg .f64 r<32>;\n");
    output.push_str("    .reg .pred p;\n\n");

    // Thread index calculation
    output.push_str("    // Calculate thread index\n");
    output.push_str("    mov.u32 %r1, %ctaid.x;\n");
    output.push_str("    mov.u32 %r2, %ntid.x;\n");
    output.push_str("    mov.u32 %r3, %tid.x;\n");
    output.push_str("    mad.lo.u32 %r4, %r1, %r2, %r3;\n");
    output.push_str("    cvt.u64.u32 idx, %r4;\n\n");

    // Bounds check
    output.push_str("    // Bounds check\n");
    output.push_str("    ld.param.u64 %rd1, [count];\n");
    output.push_str("    setp.ge.u64 p, idx, %rd1;\n");
    output.push_str(&format!("    @p bra {}_EXIT;\n\n", block.name));

    // Generate instructions from IR
    let mut reg_counter = 0;
    for instr in &block.instructions {
        output.push_str(&generate_ptx_instruction(instr, &mut reg_counter)?);
    }

    // Exit label
    output.push_str(&format!("\n{}_EXIT:\n", block.name));
    output.push_str("    ret;\n");
    output.push_str("}\n\n");

    Ok(output)
}

fn generate_hip_kernel_from_block(block: &IRBlock7D) -> Result<String, String> {
    let mut output = String::new();

    output.push_str(&format!("// Kernel: {}\n", block.name));
    output.push_str(&format!("__global__ void {}(\n", block.name));
    output.push_str("    const Vector7D* __restrict__ input,\n");
    output.push_str("    Vector7D* __restrict__ output,\n");
    output.push_str("    size_t count\n");
    output.push_str(") {\n");
    output.push_str("    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n");
    output.push_str("    if (idx >= count) return;\n\n");

    // Generate C++ code from IR
    for instr in &block.instructions {
        output.push_str(&generate_hip_instruction(instr)?);
    }

    output.push_str("}\n\n");

    Ok(output)
}

fn generate_ptx_instruction(instr: &IR7D, reg_counter: &mut usize) -> Result<String, String> {
    let output = match instr {
        IR7D::PushFloat(val) => {
            *reg_counter += 1;
            format!("    mov.f64 r{}, {:e};\n", *reg_counter, val)
        }
        IR7D::PushInt(val) => {
            *reg_counter += 1;
            format!("    mov.s64 %rd{}, {};\n", *reg_counter, val)
        }
        IR7D::Add => format!(
            "    add.f64 r{}, r{}, r{};\n",
            *reg_counter - 1,
            *reg_counter - 1,
            *reg_counter
        ),
        IR7D::Sub => format!(
            "    sub.f64 r{}, r{}, r{};\n",
            *reg_counter - 1,
            *reg_counter - 1,
            *reg_counter
        ),
        IR7D::Mul => format!(
            "    mul.f64 r{}, r{}, r{};\n",
            *reg_counter - 1,
            *reg_counter - 1,
            *reg_counter
        ),
        IR7D::Div => format!(
            "    div.rn.f64 r{}, r{}, r{};\n",
            *reg_counter - 1,
            *reg_counter - 1,
            *reg_counter
        ),
        IR7D::ManifoldProject {
            input_reg,
            output_reg,
            curvature,
        } => {
            format!(
                "    // Manifold project (curvature={})\n    call manifold_project_inline, (r{}, r{}, {:e});\n",
                curvature, input_reg, output_reg, curvature
            )
        }
        IR7D::HolographicFold {
            p1_reg,
            p2_reg,
            out_reg,
            phases,
        } => {
            format!(
                "    // Holographic fold ({} phases)\n    call holographic_fold_inline, (r{}, r{}, r{}, {});\n",
                phases, p1_reg, p2_reg, out_reg, phases
            )
        }
        _ => format!("    // {:?}\n", instr),
    };

    Ok(output)
}

fn generate_hip_instruction(instr: &IR7D) -> Result<String, String> {
    let output = match instr {
        IR7D::PushFloat(val) => format!("    double temp = {:e};\n", val),
        IR7D::PushInt(val) => format!("    int64_t itemp = {};\n", val),
        IR7D::Add => "    // Add operation\n".to_string(),
        IR7D::Sub => "    // Sub operation\n".to_string(),
        IR7D::Mul => "    // Mul operation\n".to_string(),
        IR7D::Div => "    // Div operation\n".to_string(),
        IR7D::ManifoldProject { curvature, .. } => {
            format!("    // Manifold project (curvature={})\n", curvature)
        }
        IR7D::HolographicFold { phases, .. } => {
            format!("    // Holographic fold ({} phases)\n", phases)
        }
        _ => format!("    // {:?}\n", instr),
    };

    Ok(output)
}

// ═══════════════════════════════════════════════════════════════════════════════
// TESTS
// ═══════════════════════════════════════════════════════════════════════════════

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ptx_generation() {
        let blocks = vec![];
        let ptx = generate_ptx(&blocks, (8, 0)).unwrap();

        assert!(ptx.contains(".version 7.0"));
        assert!(ptx.contains("PHI"));
        assert!(ptx.contains("manifold_project_7d"));
    }

    #[test]
    fn test_hip_generation() {
        let blocks = vec![];
        let hip = generate_hip(&blocks).unwrap();

        assert!(hip.contains("#include <hip/hip_runtime.h>"));
        assert!(hip.contains("Vector7D"));
    }
}
