//! # 7D Crystal x86-64 Backend
//!
//! Generates x86-64 assembly with SIMD optimizations for CPU execution.
//!
//! ## Features
//!
//! - AVX-512 support for 8-wide f64 operations
//! - AVX2 fallback for broader compatibility
//! - 7D vector operations optimized for cache locality
//! - Φ-ratio constants in read-only data section

use crate::ir::{IRBlock7D, IR7D};
use crate::{PHI, PHI_INV, S2_STABILITY};

// ═══════════════════════════════════════════════════════════════════════════════
// ASSEMBLY GENERATION
// ═══════════════════════════════════════════════════════════════════════════════

/// Generate x86-64 assembly from IR blocks.
pub fn generate_asm(
    blocks: &[IRBlock7D],
    use_avx512: bool,
    use_avx2: bool,
) -> Result<String, String> {
    let mut output = String::new();
    
    // Assembly header
    output.push_str("; 7D Crystal x86-64 Assembly\n");
    output.push_str("; Generated by 7D Crystal Compiler v2.0.0\n");
    output.push_str("; (c) 2025-2026 Sir Charles Spikes\n\n");
    
    if use_avx512 {
        output.push_str("; Target: x86-64 with AVX-512\n");
    } else if use_avx2 {
        output.push_str("; Target: x86-64 with AVX2\n");
    } else {
        output.push_str("; Target: x86-64 with SSE4.2\n");
    }
    output.push_str("\n");
    
    // Assembler directives
    output.push_str("section .data\n");
    output.push_str("    align 64\n");
    output.push_str(&format!("    PHI:       dq {:e}\n", PHI));
    output.push_str(&format!("    PHI_INV:   dq {:e}\n", PHI_INV));
    output.push_str(&format!("    S2_BOUND:  dq {:e}\n", S2_STABILITY));
    output.push_str("    ONE:       dq 1.0\n");
    output.push_str("    DIMS:      dd 7\n\n");
    
    // Broadcast constants for SIMD
    if use_avx512 || use_avx2 {
        output.push_str("    align 64\n");
        output.push_str(&format!("    PHI_VEC:     times 8 dq {:e}\n", PHI));
        output.push_str(&format!("    PHI_INV_VEC: times 8 dq {:e}\n", PHI_INV));
        output.push_str(&format!("    S2_VEC:      times 8 dq {:e}\n", S2_STABILITY));
        output.push_str("    ONE_VEC:     times 8 dq 1.0\n\n");
    }
    
    // Code section
    output.push_str("section .text\n");
    output.push_str("    global _start\n\n");
    
    // Built-in functions
    if use_avx512 {
        output.push_str(&generate_avx512_functions());
    } else if use_avx2 {
        output.push_str(&generate_avx2_functions());
    } else {
        output.push_str(&generate_sse_functions());
    }
    
    // Generate code from IR blocks
    for block in blocks {
        output.push_str(&generate_asm_block(block, use_avx512, use_avx2)?);
    }
    
    // Entry point
    output.push_str("_start:\n");
    output.push_str("    ; Initialize and call main\n");
    if !blocks.is_empty() {
        output.push_str(&format!("    call {}\n", blocks[0].name));
    }
    output.push_str("    ; Exit\n");
    output.push_str("    mov rax, 60\n");
    output.push_str("    xor rdi, rdi\n");
    output.push_str("    syscall\n");
    
    Ok(output)
}

// ═══════════════════════════════════════════════════════════════════════════════
// AVX-512 FUNCTIONS
// ═══════════════════════════════════════════════════════════════════════════════

fn generate_avx512_functions() -> String {
    r#"
; ═══════════════════════════════════════════════════════════════════════════════
; AVX-512 OPTIMIZED FUNCTIONS
; ═══════════════════════════════════════════════════════════════════════════════

; Calculate norm of 7D vector (AVX-512)
; Input: rdi = pointer to vector (7 doubles)
; Output: xmm0 = norm (scalar double)
norm_7d_avx512:
    push rbp
    mov rbp, rsp
    
    ; Load 7 doubles (need 2 ZMM loads, use first 7)
    vmovupd zmm0, [rdi]          ; Load 8 doubles (use 7)
    
    ; Square each element
    vmulpd zmm1, zmm0, zmm0
    
    ; Horizontal sum (reduce)
    vextractf64x4 ymm2, zmm1, 1
    vaddpd ymm1, ymm1, ymm2
    vextractf128 xmm2, ymm1, 1
    vaddpd xmm1, xmm1, xmm2
    vhaddpd xmm1, xmm1, xmm1
    
    ; Add the 7th element (already included)
    ; Square root
    vsqrtsd xmm0, xmm1, xmm1
    
    pop rbp
    ret

; Project vector to Poincaré ball (AVX-512)
; Input: rdi = input ptr, rsi = output ptr, xmm0 = curvature, rcx = count
manifold_project_7d_avx512:
    push rbp
    mov rbp, rsp
    push rbx
    
    xor rbx, rbx                  ; index = 0
    
.loop:
    cmp rbx, rcx
    jge .done
    
    ; Calculate offset
    mov rax, rbx
    shl rax, 6                    ; * 64 bytes (8 doubles, use 7)
    
    ; Load input vector
    vmovupd zmm0, [rdi + rax]
    
    ; Calculate norm
    vmulpd zmm1, zmm0, zmm0
    vextractf64x4 ymm2, zmm1, 1
    vaddpd ymm1, ymm1, ymm2
    vextractf128 xmm2, ymm1, 1
    vaddpd xmm1, xmm1, xmm2
    vhaddpd xmm1, xmm1, xmm1
    vsqrtsd xmm1, xmm1, xmm1      ; norm in xmm1
    
    ; Calculate denominator: 1 + norm + PHI_INV + |curvature|
    vbroadcastsd zmm2, [PHI_INV]
    vbroadcastsd zmm3, [ONE]
    
    ; scalar: denom = 1 + norm + PHI_INV + |curvature|
    vandpd xmm4, xmm0, [abs_mask] ; |curvature|
    vaddsd xmm1, xmm1, xmm2       ; + PHI_INV
    vaddsd xmm1, xmm1, xmm3       ; + 1
    vaddsd xmm1, xmm1, xmm4       ; + |curvature|
    
    ; scale = 1 / denom
    vdivsd xmm1, xmm3, xmm1
    
    ; Broadcast scale and multiply
    vbroadcastsd zmm1, xmm1
    vmulpd zmm0, zmm0, zmm1
    
    ; Store result
    vmovupd [rsi + rax], zmm0
    
    inc rbx
    jmp .loop
    
.done:
    pop rbx
    pop rbp
    ret

; Holographic fold (AVX-512)
; Input: rdi = p1 ptr, rsi = p2 ptr, rdx = out ptr, ecx = phases, r8 = count
holographic_fold_7d_avx512:
    push rbp
    mov rbp, rsp
    push rbx
    push r12
    
    ; Convert phases to reciprocal
    cvtsi2sd xmm0, ecx
    vmovsd xmm1, [ONE]
    vdivsd xmm0, xmm1, xmm0       ; 1.0 / phases
    vbroadcastsd zmm2, xmm0       ; broadcast phase_scale
    
    ; Load PHI_INV for modulation
    vbroadcastsd zmm3, [PHI_INV]
    vmulpd zmm2, zmm2, zmm3       ; phase_scale * PHI_INV
    
    xor rbx, rbx                   ; index = 0
    
.fold_loop:
    cmp rbx, r8
    jge .fold_done
    
    mov rax, rbx
    shl rax, 6                     ; * 64
    
    ; Load patterns
    vmovupd zmm0, [rdi + rax]     ; pattern1
    vmovupd zmm1, [rsi + rax]     ; pattern2
    
    ; Interference: (p1 + p2) * phase_scale * PHI_INV
    vaddpd zmm0, zmm0, zmm1
    vmulpd zmm0, zmm0, zmm2
    
    ; Store result
    vmovupd [rdx + rax], zmm0
    
    inc rbx
    jmp .fold_loop
    
.fold_done:
    pop r12
    pop rbx
    pop rbp
    ret

"#.to_string()
}

// ═══════════════════════════════════════════════════════════════════════════════
// AVX2 FUNCTIONS
// ═══════════════════════════════════════════════════════════════════════════════

fn generate_avx2_functions() -> String {
    r#"
; ═══════════════════════════════════════════════════════════════════════════════
; AVX2 OPTIMIZED FUNCTIONS
; ═══════════════════════════════════════════════════════════════════════════════

; Calculate norm of 7D vector (AVX2)
norm_7d_avx2:
    push rbp
    mov rbp, rsp
    
    ; Load 7 doubles in two YMM registers
    vmovupd ymm0, [rdi]           ; First 4
    vmovupd ymm1, [rdi + 32]      ; Next 3 (+ 1 garbage)
    
    ; Zero the 8th element
    vxorpd ymm2, ymm2, ymm2
    vblendpd ymm1, ymm1, ymm2, 0x8 ; Zero last element
    
    ; Square
    vmulpd ymm0, ymm0, ymm0
    vmulpd ymm1, ymm1, ymm1
    
    ; Add
    vaddpd ymm0, ymm0, ymm1
    
    ; Horizontal sum
    vextractf128 xmm1, ymm0, 1
    vaddpd xmm0, xmm0, xmm1
    vhaddpd xmm0, xmm0, xmm0
    
    ; Square root
    vsqrtsd xmm0, xmm0, xmm0
    
    pop rbp
    ret

; Project vector to Poincaré ball (AVX2)
manifold_project_7d_avx2:
    push rbp
    mov rbp, rsp
    push rbx
    
    xor rbx, rbx
    
.loop_avx2:
    cmp rbx, rcx
    jge .done_avx2
    
    mov rax, rbx
    shl rax, 6
    
    ; Load input
    vmovupd ymm0, [rdi + rax]
    vmovupd ymm1, [rdi + rax + 32]
    
    ; Calculate norm (inline)
    vmulpd ymm2, ymm0, ymm0
    vmulpd ymm3, ymm1, ymm1
    vaddpd ymm2, ymm2, ymm3
    vextractf128 xmm3, ymm2, 1
    vaddpd xmm2, xmm2, xmm3
    vhaddpd xmm2, xmm2, xmm2
    vsqrtsd xmm2, xmm2, xmm2      ; norm
    
    ; Calculate scale
    vaddsd xmm2, xmm2, [PHI_INV]
    vaddsd xmm2, xmm2, [ONE]
    vmovsd xmm3, [ONE]
    vdivsd xmm2, xmm3, xmm2
    
    ; Broadcast and multiply
    vbroadcastsd ymm2, xmm2
    vmulpd ymm0, ymm0, ymm2
    vmulpd ymm1, ymm1, ymm2
    
    ; Store
    vmovupd [rsi + rax], ymm0
    vmovupd [rsi + rax + 32], ymm1
    
    inc rbx
    jmp .loop_avx2
    
.done_avx2:
    pop rbx
    pop rbp
    ret

"#.to_string()
}

// ═══════════════════════════════════════════════════════════════════════════════
// SSE FUNCTIONS (Fallback)
// ═══════════════════════════════════════════════════════════════════════════════

fn generate_sse_functions() -> String {
    r#"
; ═══════════════════════════════════════════════════════════════════════════════
; SSE4.2 FALLBACK FUNCTIONS
; ═══════════════════════════════════════════════════════════════════════════════

; Calculate norm of 7D vector (SSE)
norm_7d_sse:
    push rbp
    mov rbp, rsp
    
    xorpd xmm0, xmm0              ; sum = 0
    
    ; Unrolled loop for 7 elements
    movsd xmm1, [rdi]
    mulsd xmm1, xmm1
    addsd xmm0, xmm1
    
    movsd xmm1, [rdi + 8]
    mulsd xmm1, xmm1
    addsd xmm0, xmm1
    
    movsd xmm1, [rdi + 16]
    mulsd xmm1, xmm1
    addsd xmm0, xmm1
    
    movsd xmm1, [rdi + 24]
    mulsd xmm1, xmm1
    addsd xmm0, xmm1
    
    movsd xmm1, [rdi + 32]
    mulsd xmm1, xmm1
    addsd xmm0, xmm1
    
    movsd xmm1, [rdi + 40]
    mulsd xmm1, xmm1
    addsd xmm0, xmm1
    
    movsd xmm1, [rdi + 48]
    mulsd xmm1, xmm1
    addsd xmm0, xmm1
    
    sqrtsd xmm0, xmm0
    
    pop rbp
    ret

; Project vector to Poincaré ball (SSE)
manifold_project_7d_sse:
    push rbp
    mov rbp, rsp
    push rbx
    push r12
    push r13
    
    mov r12, rdi                  ; input
    mov r13, rsi                  ; output
    xor rbx, rbx                  ; index
    
.loop_sse:
    cmp rbx, rcx
    jge .done_sse
    
    mov rax, rbx
    shl rax, 6
    
    ; Save for norm calculation
    lea rdi, [r12 + rax]
    call norm_7d_sse              ; norm in xmm0
    
    ; Calculate scale
    addsd xmm0, [PHI_INV]
    addsd xmm0, [ONE]
    movsd xmm1, [ONE]
    divsd xmm1, xmm0              ; scale in xmm1
    
    ; Scale each element
    mov rax, rbx
    shl rax, 6
    
    %assign i 0
    %rep 7
        movsd xmm0, [r12 + rax + i*8]
        mulsd xmm0, xmm1
        movsd [r13 + rax + i*8], xmm0
        %assign i i+1
    %endrep
    
    inc rbx
    jmp .loop_sse
    
.done_sse:
    pop r13
    pop r12
    pop rbx
    pop rbp
    ret

"#.to_string()
}

// ═══════════════════════════════════════════════════════════════════════════════
// BLOCK GENERATION
// ═══════════════════════════════════════════════════════════════════════════════

fn generate_asm_block(
    block: &IRBlock7D,
    use_avx512: bool,
    use_avx2: bool,
) -> Result<String, String> {
    let mut output = String::new();
    
    output.push_str(&format!("\n; Block: {}\n", block.name));
    output.push_str(&format!("{}:\n", block.name));
    output.push_str("    push rbp\n");
    output.push_str("    mov rbp, rsp\n");
    output.push_str("    sub rsp, 128                ; Local storage\n\n");
    
    for instr in &block.instructions {
        output.push_str(&generate_asm_instruction(instr, use_avx512, use_avx2)?);
    }
    
    output.push_str("\n    add rsp, 128\n");
    output.push_str("    pop rbp\n");
    output.push_str("    ret\n");
    
    Ok(output)
}

fn generate_asm_instruction(
    instr: &IR7D,
    use_avx512: bool,
    _use_avx2: bool,
) -> Result<String, String> {
    let output = match instr {
        IR7D::PushFloat(val) => {
            format!("    mov rax, {:016x}h\n    push rax\n", val.to_bits())
        }
        IR7D::PushInt(val) => {
            format!("    push {}\n", val)
        }
        IR7D::Add => {
            if use_avx512 {
                "    vaddsd xmm0, xmm0, xmm1\n".to_string()
            } else {
                "    addsd xmm0, xmm1\n".to_string()
            }
        }
        IR7D::Sub => {
            if use_avx512 {
                "    vsubsd xmm0, xmm0, xmm1\n".to_string()
            } else {
                "    subsd xmm0, xmm1\n".to_string()
            }
        }
        IR7D::Mul => {
            if use_avx512 {
                "    vmulsd xmm0, xmm0, xmm1\n".to_string()
            } else {
                "    mulsd xmm0, xmm1\n".to_string()
            }
        }
        IR7D::Div => {
            if use_avx512 {
                "    vdivsd xmm0, xmm0, xmm1\n".to_string()
            } else {
                "    divsd xmm0, xmm1\n".to_string()
            }
        }
        IR7D::ManifoldProject { curvature, .. } => {
            format!(
                "    ; Manifold project (curvature={})\n    call manifold_project_7d_{}\n",
                curvature,
                if use_avx512 { "avx512" } else { "avx2" }
            )
        }
        _ => format!("    ; {:?}\n", instr),
    };
    
    Ok(output)
}

// ═══════════════════════════════════════════════════════════════════════════════
// TESTS
// ═══════════════════════════════════════════════════════════════════════════════

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_asm_generation_avx512() {
        let blocks = vec![];
        let asm = generate_asm(&blocks, true, false).unwrap();
        
        assert!(asm.contains("AVX-512"));
        assert!(asm.contains("PHI:"));
    }

    #[test]
    fn test_asm_generation_avx2() {
        let blocks = vec![];
        let asm = generate_asm(&blocks, false, true).unwrap();
        
        assert!(asm.contains("AVX2"));
    }

    #[test]
    fn test_asm_generation_sse() {
        let blocks = vec![];
        let asm = generate_asm(&blocks, false, false).unwrap();
        
        assert!(asm.contains("SSE4.2"));
    }
}
